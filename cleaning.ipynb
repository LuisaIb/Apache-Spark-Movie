{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3c53c00-4d6c-4a83-83fe-928a80fa61f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cleaning | Preprocessing the given csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b7d67afe-1d82-4372-8ee8-439b212bb807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd3fa640-591b-44c4-bfea-6233e664860d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating a spark session\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"MovieAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "20a2a3ae-254c-40ec-afe9-1674d979d01f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining the absolute path to the directory where the files will be stored\n",
    "root_path = \"dbfs:/FileStore/shared_uploads/Ibele@stud.dhbw-ravensburg.de/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b36098e0-96b1-485b-899d-29404140106f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining a schema for reading in the credits.csv\n",
    "credit_schema = t.StructType([t.StructField('movie_id', t.IntegerType(), True),\n",
    "                          t.StructField('title', t.StringType(), True),\n",
    "                          t.StructField('cast', t.StringType(), True),\n",
    "                          t.StructField('crew', t.StringType(), True)])\n",
    "\n",
    "# defining a schema for reading in the movies.csv\n",
    "movie_schema = t.StructType([t.StructField('budget', t.IntegerType(), True),\n",
    "                          t.StructField('genres', t.IntegerType(), True),\n",
    "                          t.StructField('homepage', t.StringType(), True),\n",
    "                          t.StructField('id', t.IntegerType(), True),\n",
    "                          t.StructField('keywords', t.StringType(), True),\n",
    "                          t.StructField('original_language', t.StringType(), True),\n",
    "                          t.StructField('original_title', t.StringType(), True),\n",
    "                          t.StructField('overview', t.StringType(), True),\n",
    "                          t.StructField('popularity', t.FloatType(), True),\n",
    "                          t.StructField('production_companies', t.StringType(), True),\n",
    "                          t.StructField('production_countries', t.StringType(), True),\n",
    "                          t.StructField('release_date', t.TimestampType(), True),\n",
    "                          t.StructField('revenue', t.IntegerType(), True),\n",
    "                          t.StructField('runtime', t.FloatType(), True),\n",
    "                          t.StructField('spoken_languages', t.StringType(), True),\n",
    "                          t.StructField('status', t.StringType(), True),\n",
    "                          t.StructField('tagline', t.StringType(), True),\n",
    "                          t.StructField('title', t.StringType(), True)])\n",
    "\n",
    "# defining a schema for reading in the recommendations.csv\n",
    "recom_schema = t.StructType([t.StructField('movie_id', t.IntegerType(), True),\n",
    "                          t.StructField('user_id', t.IntegerType(), True),\n",
    "                          t.StructField('vote', t.IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "93f6944b-318a-4fc5-812e-5582f0e27b62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading the given csv files in a pyspark dataframe with schema and options to handle the format of the data in the csv\n",
    "credits = (spark.read\n",
    "            .option(\"header\", \"true\").option(\"escape\",'\"').option(\"mode\", \"DROPMALFORMED\").schema(credit_schema)\n",
    "            .csv(\"dbfs:/FileStore/shared_uploads/ibele@stud.dhbw-ravensburg.de/credits_groupA-1.csv\"))\n",
    "\n",
    "movies = (spark.read\n",
    "            .option(\"header\", \"true\").option(\"escape\",'\"').option(\"mode\", \"DROPMALFORMED\")\n",
    "            .csv(\"dbfs:/FileStore/shared_uploads/ibele@stud.dhbw-ravensburg.de/movies_groupA-1.csv\"))\n",
    "\n",
    "recoms = (spark.read\n",
    "            .option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").schema(recom_schema)\n",
    "            .csv(\"dbfs:/FileStore/shared_uploads/ibele@stud.dhbw-ravensburg.de/recommendations_groupA-1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "254fe724-79a0-49f6-a695-222f8d9f085e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# converting the movie and credits pyspark dataframe to a pandas dataframe for the cleaning process later\n",
    "df_movies = movies.toPandas()\n",
    "df_credits = credits.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0313e988-b0c2-45d0-998e-a22f63fcc044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_normalized_df(df: pd.DataFrame, col: str, key_attribute: str=\"movie_id\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    function for cleaning and normalizing the given data in the pandas dataframes\n",
    "    this function doesn't check for the completeness and validity of rows in the dataset to prevent potential loss of data\n",
    "    \n",
    "    attributes: \n",
    "        df (pd.Dataframe): dataframe to be normalized\n",
    "        col (str): column of the dataframe to be normalized\n",
    "        key_attribute (str): key attribute of dataframe\n",
    "    \"\"\"\n",
    "    # create copy of the dataframe, so the original doesn't get alternated\n",
    "    df_copy = df.copy() \n",
    "    \n",
    "    df_copy[col] = (df_copy[col] # select column of interest\n",
    "                    .map(json.loads) # read as json\n",
    "                    .map(pd.json_normalize)) # create normalized dataframes\n",
    "    \n",
    "    # assign key of the row to the newly created normalized dataframes\n",
    "    clean_dfs = [row[col].assign(id=row[key_attribute]) for _, row in df_copy.iterrows()] \n",
    "    \n",
    "    # join normalized dataframes\n",
    "    df_clean = pd.concat(clean_dfs, ignore_index=True)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# list of the to be created dataframes: (column, source dataframe, key attribute)\n",
    "new_dataframes = [\n",
    "    (\"production_companies\", df_movies, \"id\"),\n",
    "    (\"spoken_languages\", df_movies, \"id\"),\n",
    "    (\"genres\", df_movies, \"id\"),\n",
    "    (\"cast\", df_credits, \"movie_id\"),\n",
    "    (\"crew\", df_credits, \"movie_id\")\n",
    "]\n",
    "\n",
    "# looping over new_dataframes in order to normalize and save as parquet file for group and individual tasks\n",
    "for column, source_df, key_attribute in new_dataframes:\n",
    "    normalized_df = get_normalized_df(source_df, column, key_attribute)\n",
    "    normalized_df = spark.createDataFrame(normalized_df)\n",
    "    normalized_df.write.format(\"parquet\").mode(\"overwrite\").save(f\"{root_path}filtered_data/{column}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21d25bbf-600f-480d-856c-d190beb09075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# since recoms is already clean there is no cleaning and no converting to pandas needed -> saving recoms as parquet file\n",
    "recoms.write.format(\"parquet\").mode(\"overwrite\").save(f\"{root_path}filtered_data/recom.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab3da37f-e08e-4685-9b87-cc1b4ad9a3a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# selecting the remaining needed columns of movie and saving them in a parquet file\n",
    "(movies\n",
    "    .select(\"title\", \"id\", \"budget\", \"revenue\", \"release_date\", \"runtime\", \"popularity\")\n",
    "    .write.format(\"parquet\").mode(\"overwrite\")\n",
    "    .save(f\"{root_path}filtered_data/movies.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eedbc0dc535c48737cd6555de2640d5e12a8c15981dff35e4ce3fb5ec7467d95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
